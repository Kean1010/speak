<script>
  const video = document.getElementById('video');
  const canvas = document.getElementById('canvas');
  const context = canvas.getContext('2d');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const statusDiv = document.getElementById('status');
  const resultsDiv = document.getElementById('results');

  let smilingSeconds = 0;
  let notSmilingSeconds = 0;
  let intervalId = null;
  let recognition = null;
  let fillerWords = { um: 0, argh: 0 };

  // Load Face-API models with error handling
  async function loadModels() {
    try {
      statusDiv.textContent = 'Loading models...';
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/'),
        faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/')
      ]);
      console.log('Models loaded successfully');
      statusDiv.textContent = 'Models loaded';
    } catch (err) {
      console.error('Error loading models:', err);
      statusDiv.textContent = 'Error: Failed to load facial detection models.';
      throw err;
    }
  }

  // Start video stream with error handling
  async function startVideo() {
    try {
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('getUserMedia not supported in this browser');
      }
      const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
      video.srcObject = stream;
      console.log('Video stream started');
    } catch (err) {
      console.error('Error accessing camera/mic:', err);
      statusDiv.textContent = 'Error: Could not access camera or microphone. Check permissions.';
      throw err;
    }
  }

  // Analyze facial expressions
  async function analyzeExpression() {
    try {
      const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceExpressions();
      if (detections) {
        const smileScore = detections.expressions.happy;
        if (smileScore > 0.5) {
          smilingSeconds += 1;
          statusDiv.textContent = 'Status: Smiling (Good)';
        } else {
          notSmilingSeconds += 1;
          statusDiv.textContent = 'Status: Not Smiling (Not Good)';
        }
      } else {
        statusDiv.textContent = 'Status: No face detected';
      }
    } catch (err) {
      console.error('Error analyzing expression:', err);
      statusDiv.textContent = 'Status: Error during analysis';
    }
  }

  // Start speech recognition
  function startSpeechRecognition() {
    if (!('SpeechRecognition' in window) && !('webkitSpeechRecognition' in window)) {
      statusDiv.textContent = 'Error: Speech recognition not supported in this browser';
      return;
    }
    recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.onresult = (event) => {
      const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase();
      if (transcript.includes('um')) fillerWords.um += 1;
      if (transcript.includes('argh')) fillerWords.argh += 1;
    };
    recognition.onerror = (err) => {
      console.error('Speech recognition error:', err);
      statusDiv.textContent = 'Status: Speech recognition error';
    };
    recognition.start();
    console.log('Speech recognition started');
  }

  // Stop and calculate results
  function stopAnalysis() {
    clearInterval(intervalId);
    if (recognition) recognition.stop();
    if (video.srcObject) {
      video.srcObject.getTracks().forEach(track => track.stop());
    }

    const totalSeconds = smilingSeconds + notSmilingSeconds;
    const smilePercentage = totalSeconds > 0 ? (smilingSeconds / totalSeconds) * 100 : 0;
    const category = smilePercentage > 50 ? 'Good (Mostly Smiling)' : 'Not Good (Mostly Not Smiling)';

    resultsDiv.innerHTML = `
      <h2>Results</h2>
      <p>Total Time: ${totalSeconds} seconds</p>
      <p>Smiling: ${smilingSeconds} seconds (${smilePercentage.toFixed(2)}%)</p>
      <p>Not Smiling: ${notSmilingSeconds} seconds (${(100 - smilePercentage).toFixed(2)}%)</p>
      <p>Category: ${category}</p>
      <p>Filler Words - Um: ${fillerWords.um}, Argh: ${fillerWords.argh}</p>
    `;

    startBtn.disabled = false;
    stopBtn.disabled = true;
    statusDiv.textContent = 'Status: Analysis stopped';
  }

  // Start button click
  startBtn.onclick = async () => {
    try {
      await loadModels();
      await startVideo();
      startSpeechRecognition();

      intervalId = setInterval(analyzeExpression, 1000); // Check every second
      startBtn.disabled = true;
      stopBtn.disabled = false;
      statusDiv.textContent = 'Status: Analyzing...';
    } catch (err) {
      console.error('Start error:', err);
    }
  };

  // Stop button click
  stopBtn.onclick = stopAnalysis;
</script>
