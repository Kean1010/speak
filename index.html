<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Smile and Speech Analyzer</title>
  <script src="https://unpkg.com/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.9.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet@2.2.2/dist/posenet.min.js"></script>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      margin: 20px;
    }
    #video, #canvas {
      display: inline-block;
      max-width: 100%;
    }
    #canvas {
      display: none;
    }
    button {
      padding: 10px 20px;
      margin: 5px;
      font-size: 16px;
    }
    #status, #results {
      margin-top: 20px;
      font-size: 18px;
    }
    #loading-pie {
      width: 50px;
      height: 50px;
      border-radius: 50%;
      background: conic-gradient(#4caf50 0% 0%, #ddd 0% 100%);
      display: inline-block;
      margin: 10px auto;
    }
  </style>
</head>
<body>
  <h1>Smile and Speech Analyzer</h1>
  <div id="loading-pie"></div>
  <p id="loading-text">Loading...</p>
  <video id="video" width="640" height="480" autoplay muted></video>
  <canvas id="canvas" width="640" height="480"></canvas>
  <br>
  <button id="startBtn" disabled>Start Analysis</button>
  <button id="stopBtn" disabled>Stop Analysis</button>
  <div id="status">Status: Waiting to start...</div>
  <div id="results"></div>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const context = canvas.getContext('2d');
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const statusDiv = document.getElementById('status');
    const resultsDiv = document.getElementById('results');
    const loadingPie = document.getElementById('loading-pie');
    const loadingText = document.getElementById('loading-text');

    let smilingSeconds = 0;
    let notSmilingSeconds = 0;
    let yawningSeconds = 0;
    let slouchingSeconds = 0;
    let intervalId = null;
    let recognition = null;
    let fillerWords = { um: 0, argh: 0, uh: 0, like: 0, 'you know': 0 };
    let net;

    let loadingProgress = 0;
    function updateLoadingProgress(percent) {
      loadingProgress = percent;
      loadingPie.style.background = `conic-gradient(#4caf50 0% ${percent}%, #ddd ${percent}% 100%)`;
      if (percent >= 100) {
        loadingText.textContent = 'Ready!';
        startBtn.disabled = false;
      }
    }

    if (typeof faceapi === 'undefined') {
      console.error('Face-API.js not loaded');
      statusDiv.textContent = 'Error: Face-API.js failed to load';
      updateLoadingProgress(0);
    } else {
      console.log('Face-API.js loaded successfully');
      updateLoadingProgress(20);
    }

    async function loadModels() {
      if (typeof faceapi === 'undefined') {
        throw new Error('Face-API.js not available');
      }
      try {
        statusDiv.textContent = 'Loading Face-API models...';
        console.log('Loading Face-API models...');
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/'),
          faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/'),
          faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/')
        ]);
        console.log('Face-API models loaded successfully');
        updateLoadingProgress(60);

        statusDiv.textContent = 'Loading PoseNet...';
        net = await posenet.load();
        console.log('PoseNet loaded successfully');
        updateLoadingProgress(100);
        statusDiv.textContent = 'Models loaded';
      } catch (err) {
        console.error('Model loading failed:', err.message);
        statusDiv.textContent = 'Error: Failed to load models';
        throw err;
      }
    }

    async function startVideo() {
      try {
        console.log('Requesting camera and microphone access...');
        statusDiv.textContent = 'Status: Requesting camera/mic...';
        const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        video.srcObject = stream;
        console.log('Video and audio stream started');
        statusDiv.textContent = 'Status: Stream started';
      } catch (err) {
        console.error('Stream error:', err.message);
        statusDiv.textContent = `Error: ${err.message}`;
        throw err;
      }
    }

    async function analyzeExpression() {
      try {
        console.log('Starting face analysis...');
        const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceExpressions();
        if (detections) {
          console.log('Face detected');
          const smileScore = detections.expressions.happy;
          if (smileScore > 0.5) {
            smilingSeconds += 1;
            statusDiv.textContent = 'Status: Smiling (Good)';
          } else {
            notSmilingSeconds += 1;
            statusDiv.textContent = 'Status: Not Smiling (Not Good)';
          }

          console.log('Checking yawning...');
          const landmarks = detections.landmarks;
          const mouthTop = landmarks.getMouth()[0];
          const mouthBottom = landmarks.getMouth()[6];
          const mouthHeight = mouthBottom.y - mouthTop.y;
          if (mouthHeight > 40) {
            yawningSeconds += 1;
            statusDiv.textContent = 'Status: Yawning Detected';
            console.log('Yawning detected, mouth height:', mouthHeight);
          }
        } else {
          statusDiv.textContent = 'Status: No face detected';
          console.log('No face detected');
        }

        console.log('Starting pose analysis...');
        const pose = await net.estimateSinglePose(video);
        const leftShoulder = pose.keypoints.find(k => k.part === 'leftShoulder');
        const rightShoulder = pose.keypoints.find(k => k.part === 'rightShoulder');
        const leftHip = pose.keypoints.find(k => k.part === 'leftHip');
        if (leftShoulder.score > 0.5 && rightShoulder.score > 0.5 && leftHip.score > 0.5) {
          const shoulderAngle = Math.abs(leftShoulder.position.y - rightShoulder.position.y);
          const shoulderToHip = leftShoulder.position.y - leftHip.position.y;
          if (shoulderAngle < 20 && shoulderToHip > 50) {
            slouchingSeconds += 1;
            statusDiv.textContent = 'Status: Slouching Detected';
            console.log('Slouching detected');
          }
        }
      } catch (err) {
        console.error('Analysis error:', err.message, err.stack);
        statusDiv.textContent = 'Status: Analysis error - Check console';
      }
    }

    function startSpeechRecognition() {
      if (!('SpeechRecognition' in window) && !('webkitSpeechRecognition' in window)) {
        console.warn('Speech recognition not supported');
        statusDiv.textContent = 'Warning: Speech recognition not supported';
        return;
      }
      recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.onresult = (event) => {
        const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase();
        if (transcript.includes('um')) fillerWords.um += 1;
        if (transcript.includes('argh')) fillerWords.argh += 1;
        if (transcript.includes('uh')) fillerWords.uh += 1;
        if (transcript.includes('like')) fillerWords.like += 1;
        if (transcript.includes('you know')) fillerWords['you know'] += 1;
      };
      recognition.onerror = (err) => {
        console.error('Speech recognition error:', err);
      };
      recognition.onend = () => {
        if (intervalId) recognition.start();
      };
      recognition.start();
      console.log('Speech recognition started');
    }

    function resetAnalysis() {
      smilingSeconds = 0;
      notSmilingSeconds = 0;
      yawningSeconds = 0;
      slouchingSeconds = 0;
      fillerWords = { um: 0, argh: 0, uh: 0, like: 0, 'you know': 0 };
      resultsDiv.innerHTML = '';
    }

    function stopAnalysis() {
      try {
        if (intervalId) {
          clearInterval(intervalId);
          intervalId = null;
          console.log('Interval cleared');
        }
        if (recognition) {
          recognition.stop();
          recognition = null;
          console.log('Speech recognition stopped');
        }
        if (video.srcObject) {
          video.srcObject.getTracks().forEach(track => track.stop());
          video.srcObject = null;
          console.log('Video stream stopped');
        }

        const totalSeconds = smilingSeconds + notSmilingSeconds;
        const smilePercentage = totalSeconds > 0 ? (smilingSeconds / totalSeconds) * 100 : 0;
        const category = smilePercentage > 50 ? 'Good (Mostly Smiling)' : 'Not Good (Mostly Not Smiling)';

        resultsDiv.innerHTML = `
          <h2>Results</h2>
          <p>Total Time: ${totalSeconds} seconds</p>
          <p>Smiling: ${smilingSeconds} seconds (${smilePercentage.toFixed(2)}%)</p>
          <p>Not Smiling: ${notSmilingSeconds} seconds (${(100 - smilePercentage).toFixed(2)}%)</p>
          <p>Yawning: ${yawningSeconds} seconds</p>
          <p>Slouching: ${slouchingSeconds} seconds</p>
          <p>Category: ${category}</p>
          <p>Filler Words - Um: ${fillerWords.um}, Argh: ${fillerWords.argh}, Uh: ${fillerWords.uh}, Like: ${fillerWords.like}, You Know: ${fillerWords['you know']}</p>
        `;

        startBtn.disabled = false;
        stopBtn.disabled = true;
        statusDiv.textContent = 'Status: Ready to start new analysis';
        console.log('Analysis stopped, ready for new start');
      } catch (err) {
        console.error('Stop error:', err.message);
        statusDiv.textContent = 'Error: Failed to stop analysis';
      }
    }

    startBtn.onclick = async () => {
      console.log('Start button clicked');
      try {
        if (typeof faceapi === 'undefined') {
          throw new Error('Face-API.js not loaded');
        }
        resetAnalysis();
        await startVideo();
        startSpeechRecognition();

        intervalId = setInterval(analyzeExpression, 1000);
        startBtn.disabled = true;
        stopBtn.disabled = false;
        statusDiv.textContent = 'Status: Analyzing...';
        console.log('Analysis started');
      } catch (err) {
        console.error('Start error:', err.message);
        statusDiv.textContent = `Error: ${err.message}`;
      }
    };

    stopBtn.onclick = stopAnalysis;

    console.log('Page loaded at', new Date().toLocaleString());
    loadModels().catch(err => {
      console.error('Initial load error:', err);
      statusDiv.textContent = 'Error: Initial setup failed';
      updateLoadingProgress(0);
    });
  </script>
</body>
</html>
